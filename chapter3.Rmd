---
title: "Sampling Statistics and Model Training in R"
author: "Kate Becker"
date: "2024-09-09"
output: html_document
---

```{r}
library(tidyverse)
```


## General Terms 
Population:  entire collection (or universe) of things under consideration 
Sample: a portion of the population we select for the analysis 

ex: polling data is an example of sampling and is gathered by asking questions of people for specific deomographics 
- the polling data can only be a subset of the general population of a country becauase it would be imposible to poll every single person, if we have a country of 300 million people and only 10 million answered the poll this is sampling 
- to fully understand what everyone in the country prefers we might have to do some extrapolation from the sample to the population 
- in the world of statistics we have values assocaited with the total population adn those associated with smaller populations 

Values related to the terms: mean, variance, and standard deviation in terms of the whole population these are called *parameters* but when talk about these but for a certain subset of the data they're *statistics*
- So we might be looking at a specific subset of a country and look at the mean statistics in that case comparing is to the mean parameter 
- the number of people in a counry who like blue would be a parameter but number of people in a particular city would be a statistic 

Bias
- sampling biase iswhat happens when you sample data in such a way that distirbutions in the samples of the data dont match up from the populations in which youre drawing from 
- sample variation is the extent to which a sample statistic (maybe favorite food as opposed to color) differs from the population 
- both of these can be controlled by picking the right way of sampling our data 


## Low bias, low variance: The best case scenario, samples are pretty well representative of the population (points are clustered and on target)

## High bias, low variance: the samples are consistent but not reflective of the entire population (points are clustered together but not on target)

## Low bias, high variance : the sample veary wildy in their consistency but some might be representative of the population (points arent clustered and are very scattered)

## High bias, high variance: the samples are a little more consistent but not likely to be representative of the entire population (points arent as clustered and arent on target)

*low bias (samples are pretty representative) low variance: samples are consistent 

#1. 
A *simple random sample* is one way of controlling bias when pulling samples from a population statement
- this is when you select values from your data at random such that every row has an equal chance of being selected 
- applying a simple random sample to the same data twice will have possibility of selecting the same data, if its truly random 

#2
A *stratified random sample* or oversampling is when you separate the data into mutually exlusive groups, called strata, and then do a simple random sample on each stratum and this would belike polling randomly across each state 
- the two advantages: 
  - ensurs representation in each strata 
  - can be more accurate than a simple random sample if there is mroe variation in one strata than others 

When you randomly select from various strata in your data (a strata could be a grouping or cut in the data that separates one part of it from another which could be due to classification or factor variables in the data)  
  
#3
The samples are spread geographically or spatially and you could perform a *cluster* sample, for examples when you have data that is stratified by country or city and this is similar to performing a stratified random sample but **picking the entire strata randomly** instead of doing a simpler random **sample within the strata**

When you take all data points from a given class or cut in the data, where the classes or cuts themselves are randomly selected

#4
A *systematic sample* is when you randomly select from your first n data points, and then select every nth data point thereafter,, this isnt random (other than the initial randomization to find the seed on which to iterate) but easy to perform on databases 


**In almost all cases you will use a simple random sample for speed and ease in implementation however certain cases may require you to stratify the data first before sampling or if data is arranged in such a way like being distributed over geographic regions you may prefer clustering 
**Note: taking 100% of your population for data isnt the best approach but need to strike a balane so that your sample has enough data points to be statistically significant and well represenative of the population statistics you're looking at 

# Sampling in R :
## Simple
```{r}
iris.df <- data.frame(iris)
sample.index <- sample(1:nrow(iris.df), nrow(iris) * 0.75, replace = FALSE)
head(iris[sample.index,])

# This code does a simple random saple of the iris dataset by first generating indices by which you need to subset your dataset and in this case we randomly select 5 rows without replacement 
# If selected replace = TRUE, when you randomly draw out a row from the data, you have the change of drawing the same row again 
```
## Stratified 
```{r}
summary(iris)
# Petal length has the highest value of variance follwed by sepal.length 
```
This sample takes 75% sample of the original data, and you can see that the distributions are all pretty close to what the population values are 
```{r}
# We intend to get a sample that has roughly the same distribution of values for any of these features but some of these columns vary to a high degree than others 

summary(iris[sample.index,])

```
```{r}
# can use the stratified functio but not available with my version of R 
library(dplyr)
library(rsample)
set.seed(123)
strata <- initial_split(iris, prop = 0.7, strata = "Sepal.Length")

train_data <- training(strata)
test_data <- testing(strata)

# View the first few rows of the training data
head(train_data)

summary(train_data)
# The stratifed sample has just about the same value
```

```{r}
# but you can you can specify which particular strata that I want to sample over, if youre sampling over many strata you generally want to start with the features that vary the least and then work way upward 
# those with the lowest variance include sepal.width and sepal.length


# again, should use stratifed function but not compatible with my R
iris <- iris %>%
  mutate(Combined_Strata = paste(Sepal.Width, Petal.Width, sep = "_"))

# Perform stratified random sampling using the combined strata
set.seed(123) # Set seed for reproducibility

split <- initial_split(iris, prop = 0.7, strata = Combined_Strata)

# Extract training (70%) and testing (30%) datasets
train_data <- training(split)
test_data <- testing(split)

# View the first few rows of the training data
head(train_data)

summary(train_data)


```

The means and variances returned here look pretty close to the population data!

# Systematic Sampling
## You can write a function that selects every nth row sequentially given some random intialization number 

```{r}
sys.saample = function(N, n) { # define function named sys.sample that takes the arguments N: total population size and n: desired sample size 
  k = ceiling(N/n) # (calculates the interval length) k, which is the step size for systematic sampling, k is computed as the ceiling of the ratio of the population size (N) to the sample size (n), using ceiling() ensures that k is rounded to the nearest whole number which avoids skipping elements when N isnt perfectly divisible by n  
  r = sample(1:k,1) # (random start point) randomly selects a starting point r from the first k element, this picks one random integer from the sequence 1 to k, setting the random start within the first interval  
  sys.sample = seq(r, r + k *(n-1), k) # generates sample sequence using the starting point r, interval length k, and sample size n , seq(r, r+k*(n-1), k) creates a sequence starting a r ending at r + k * (n-1) with a step size of k
} # the sequence selects every kth element starting from r, resulting in a sample of size n 

# ceiling takes a singe numeric argument x and returns a numeric vector containing the smallest integers not less than corresponding elements of x 
```

Example Usage:
If you call sys.sample(100, 10), assuming N = 100 and n = 10:

k = ceiling(100/10) = 10
Suppose r = 3 is randomly selected.
The systematic sample sequence would be: seq(3, 3 + 10 * (10-1), 10), which evaluates to seq(3, 93, 10), resulting in [3, 13, 23, 33, 43, 53, 63, 73, 83, 93].



#Example Usage:
If you call 
sys.sample(100, 10) #assuming N = 100 and n = 10:

k = ceiling(100/10) = 10
Suppose r = 3 is randomly selected.
The systematic sample sequence would be: seq(3, 3 + 10 * (10-1), 10), which evaluates to seq(3, 93, 10), resulting in [3, 13, 23, 33, 43, 53, 63, 73, 83, 93].

# Training and testing 
When building a predictive model you need to see what the errors generated by the model are so that you can tune it appropriately 

## Two major assumptions when working with training/testing splits 
- The data is a fair representation of the actual processes that you want to model (the subset accurately reflects the population)
- The process that you want to model are relatively stable over time and that a model built with last months data should accurately reflect next months data 

## Roles of Training and Testing Sets
- Almost all unsupervised learning algorithms follow the format for splitting the data into a training and testing set and use the training set for model training 
  - The coefficients you get as a result of the modeling procedures are based entirely on thet training data and dont depend on the testing at all 

## Why make a test set?
- The valule of making a test set of data for modelling purposes is extremely important because if it worked well for training but crashed with testing it loses all of its predictive power and becomes nothing more than a statc report 
  - "must tweek parameter B by a small amout to fit it"
  
- *Classification and regressiont regression trees (CARTS)* can be so flexibile in their modelling capabilities that if the tree is large enough you can often get misleading predictions 
  - you might train a model and see that it gives you 100% accuracy which should cause of concern , you can use the test data to evaluate the predictive performance of the trees in the data to find the one with the lowest error 
*the test set acts not only to validate the data but as a way to select which form of the model you need depending on the algorithm at play* 


## Training and Test Sets: Regression Modelling 

```{r}
set.seed(123)
x <- rnorm(100,2,1) #generate random normal data , 100 random numbers from a normal distribution with a mean of 2 and sd of 1. variable x is assigned this vector of random numbers 
y = exp(x) + rnorm(5, 0, 2) # creates a response variable y using a transformation of x , exp(x) calculates the exponential of each value in x, transforming it non-linearly 
plot(x,y)

linear <- lm(y~x) #fitting the linear regression model 
abline(a = coef(linear[1], b = coef(linear[2], lty = 2))) # adding regression line using the intercept and slope from the linear model, lty sets type of line to dashed  

# randomized data with a linear fit attached, the linear fit comes close to fitting some data points but not all (the further you extend x out, the more likely it is that your linear fit wont approximate the data very well )


```
```{r}
summary(linear)
```

The above summary uses 100% of the simulated data as its training and looks at model performance, a R squared of 0.74 isnt great but lets try to split 70/30!

```{r}
data <- data.frame(x,y)
data.samples <- sample(1:nrow(data), nrow(data) * .70, replace = FALSE)

training.data <- data[data.samples,] #training everything in 70%
test.data <- data[-data.samples,] #testing everything in 30%
```

```{r}
train.linear <- lm(y ~ x, training.data) # running the linear regression on training
```

```{r}
train.output <- predict(train.linear, test.data) # predict the linear model on test data
```
 Here your model is comparing what the model thinks the answer should be given input x compared to the actual values in your test set given by y..for regression...depending on the data and waht kind of error analysis that you want to do specifically...use **root-mean-square error (RMSE)**
 
# RMSE
You take the output values that the model has provided for the training data input, subtract those by the y values that you have in the test data, square the values, divide those by the total number of observations and sum up all values, and take the square root
 
```{r}
RMSE.df = data.frame(predicted = train.output, actual = test.data$y,
                     SE = ((train.output - test.data$y)^2/length(train.output)))

head(RMSE.df)
```

 
```{r}
sqrt(sum(RMSE.df$SE))
```
 
The value of 6.9 for RMSE is this models error score. To see how good this number is compare it another RMSE. You can run the same logic on a function fit of one higher degree and see what kind of RMSE you get as an end result.

```{r}
train.quadratc <- lm(y ~ x^2 + x, training.data) #one higher degree 
quadratic.output <- predict(train.quadratc, test.data)
RMSE.quad.df = data.frame(predicted = quadratic.output, actual = test.data$y,
                          SE = ((quadratic.output - test.data$y)^2)/length(train.output))

head(RMSE.quad.df)

```
 

```{r}
sqrt(sum(RMSE.quad.df$SE))
```
 
The output shows that bumping up the polynomial degree fit by one to a quadratic helps to decrease the error in what the model is predicting (from the quadratic,output variable) compared to what the actual values are. This follows from the fact that the actual data you're plotting appears to fit very well by a quadratic anyway. 
 
The next step is to increase the polynomial degree even further and assess how it affects the RMSE:

```{r}
train.polyn <- lm(y~poly(x,4), training.data)
polyn.output <- predict(train.polyn, test.data)
RMSE.polyn.df = data.frame(predicted = polyn.output, actual = test.data$y,
                           SE = ((polyn.output - test.data$y)^2/length(train.output)))
head(sum(RMSE.polyn.df$SE))
```


The RMSE has gone up compared ot the quadratic fit case. This follows the same pattern of a higher degree polynomial overfitting the data. But a smaller RMSE shows that the model fits the data nicely and morea ccurate predictions.

A simple linear fit to data would mostly be under fit; this is when the fitted line as given by the coefficients output from the model dont quite line up with the data you have. Meaning that the line on the chart the represents the machinel earning model (simple linear regression) doesnt explain most of the data, its too simple a model. Difficult for linear regressionto overfit the model so we rarely use training and testing data to evaluate them. With a quadratic fit the model tends to conform to the shape of most of the data points and now the model is a bit more complex. If you were to add anoter couple of data points in the same shape it would look like fits well. 
  - By training a  ML model with a specific training sample and looking at the difference between that and the saved test data you can evaluate how well the ML fit the data 
  - The downside of this future explanatory power if we add a few more dat apoints the model shown wont fit them well at all and will have an increased error 
  
  
In a complex model fit scenario, where the model is too specific to the training data, new test data will most likely have a lot of errors when we evaluated the model output versus our test data 

- When you do a train/test validation on continuous data as in regression, you can choose from a series of statistical measures like RMSE but you want to compare the output values that the mdoel gives you 
- But you want to compare the output values that the model gives you based on a subset of data that you used to train the model to that of the data you held out for testing purposes 
- Should have a list of numbers for the model estimates and a list of numbers that are the actual values 
- Thank you can bubble up to some aggregate number and compare against other methods 

## Root mean square error(RMSE), Mean absolute error(MAE), Root relative square error(RRSE), Relative Absolute Error (RAE)
- For **RMSE and MAE**, we look at the average difference between the model output ypredicted and the values we have in our test set yactual. 
- These are compared to the same scale of our feature
- You can think of it like 1 point of error is a difference of 1 between predicted and yactual 
- We tend to multiply it by 100 to get something in the 1 to 100 range and convert it to a percentage as well 
- The denominators of the two equations tells us how much the feature deviates from its average value which is why they call them relative 
# Training and Test Sets: Classification Modeling 
You evaluate a classifications models performance by starting with a confusion matrix. In a simple form, it can take representation as a 2 x 2 matrix in whcih the model output predicted classes are compared to the actual classes and the count of the model output in the cells of the matrix 
**This informs us how many true positives, true negatives, false positives, and false negatives there are**

```{r}
iris.df <- iris
iris.df$Species <- as.character(iris.df$Species) # convert species feature to a character
iris.df$Species[iris.df$Species != "setosa"] <- "other" # anything in species feature not setosa name as other
iris.df$Species <- as.factor(iris.df$Species)
iris.samples <- sample(1:nrow(iris.df), nrow(iris.df) * 0.7, # take a sample thats 70%
                       replace = TRUE)
training.iris <- iris.df[iris.samples,]
testing.iris <- iris.df[-iris.samples, ]
```


```{r}
library(randomForest)
iris.rf <- randomForest(Species ~ ., data = training.iris)
iris.predictions <- predict(iris.rf, testing.iris)
table(iris.predictions, testing.iris$Species)
# In a binary class truth table there are two outcomes: either predicted values is some class or it is not, in this case you're focusing on whether the model predicted setosa class or something else, there are four values for the confusion table

# Below our model is extremely accurate 
```
## True Positive
The model predicted setosa class and got them right 
## True Negative
The model predicted other classes and got them right 
## False Positive
The model predicted setosa and got it wrong
## False Negative
The model predicted other classes and got it wrong 

- 54 is true postive
- 0 is false negative (bottom left)
- 0 is false positive (top right)
- 27 is true negative 

Predicted is at top of matrix and actual is vertical left of matrix 

Say that other that (moving left to right top to bottom it was 28, 3, 2, 12) This forcees two false positives and two false negatives, we have 15 true positives (TP), 26 true negatives (TN), 2 false positives (FP), and 2 false negatives (FN)

- 28 is true positive 
- 2 is false negative 
- 3 is false positive
- 12 is true negative 

*now 15 true positives (3 + 12), 26 true negatives (28-2), 2 false positives, and 3 false negatives  

