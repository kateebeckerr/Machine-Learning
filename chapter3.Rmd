---
title: "Sampling Statistics and Model Training in R"
author: "Kate Becker"
date: "2024-09-09"
output: html_document
---

```{r}
library(tidyverse)
```


## General Terms 
Population:  entire collection (or universe) of things under consideration 
Sample: a portion of the population we select for the analysis 

ex: polling data is an example of sampling and is gathered by asking questions of people for specific deomographics 
- the polling data can only be a subset of the general population of a country becauase it would be imposible to poll every single person, if we have a country of 300 million people and only 10 million answered the poll this is sampling 
- to fully understand what everyone in the country prefers we might have to do some extrapolation from the sample to the population 
- in the world of statistics we have values assocaited with the total population adn those associated with smaller populations 

Values related to the terms: mean, variance, and standard deviation in terms of the whole population these are called *parameters* but when talk about these but for a certain subset of the data they're *statistics*
- So we might be looking at a specific subset of a country and look at the mean statistics in that case comparing is to the mean parameter 
- the number of people in a counry who like blue would be a parameter but number of people in a particular city would be a statistic 

Bias
- sampling biase iswhat happens when you sample data in such a way that distirbutions in the samples of the data dont match up from the populations in which youre drawing from 
- sample variation is the extent to which a sample statistic (maybe favorite food as opposed to color) differs from the population 
- both of these can be controlled by picking the right way of sampling our data 


## Low bias, low variance: The best case scenario, samples are pretty well representative of the population (points are clustered and on target)

## High bias, low variance: the samples are consistent but not reflective of the entire population (points are clustered together but not on target)

## Low bias, high variance : the sample veary wildy in their consistency but some might be representative of the population (points arent clustered and are very scattered)

## High bias, high variance: the samples are a little more consistent but not likely to be representative of the entire population (points arent as clustered and arent on target)

*low bias (samples are pretty representative) low variance: samples are consistent 

#1. 
A *simple random sample* is one way of controlling bias when pulling samples from a population statement
- this is when you select values from your data at random such that every row has an equal chance of being selected 
- applying a simple random sample to the same data twice will have possibility of selecting the same data, if its truly random 

#2
A *stratified random sample* or oversampling is when you separate the data into mutually exlusive groups, called strata, and then do a simple random sample on each stratum and this would belike polling randomly across each state 
- the two advantages: 
  - ensurs representation in each strata 
  - can be more accurate than a simple random sample if there is mroe variation in one strata than others 

When you randomly select from various strata in your data (a strata could be a grouping or cut in the data that separates one part of it from another which could be due to classification or factor variables in the data)  
  
#3
The samples are spread geographically or spatially and you could perform a *cluster* sample, for examples when you have data that is stratified by country or city and this is similar to performing a stratified random sample but **picking the entire strata randomly** instead of doing a simpler random **sample within the strata**

When you take all data points from a given class or cut in the data, where the classes or cuts themselves are randomly selected

#4
A *systematic sample* is when you randomly select from your first n data points, and then select every nth data point thereafter,, this isnt random (other than the initial randomization to find the seed on which to iterate) but easy to perform on databases 


**In almost all cases you will use a simple random sample for speed and ease in implementation however certain cases may require you to stratify the data first before sampling or if data is arranged in such a way like being distributed over geographic regions you may prefer clustering 
**Note: taking 100% of your population for data isnt the best approach but need to strike a balane so that your sample has enough data points to be statistically significant and well represenative of the population statistics you're looking at 

# Sampling in R :
## Simple
```{r}
iris.df <- data.frame(iris)
sample.index <- sample(1:nrow(iris.df), nrow(iris) * 0.75, replace = FALSE)
head(iris[sample.index,])

# This code does a simple random saple of the iris dataset by first generating indices by which you need to subset your dataset and in this case we randomly select 5 rows without replacement 
# If selected replace = TRUE, when you randomly draw out a row from the data, you have the change of drawing the same row again 
```
## Stratified 
```{r}
summary(iris)
# Petal length has the highest value of variance follwed by sepal.length 
```
This sample takes 75% sample of the original data, and you can see that the distributions are all pretty close to what the population values are 
```{r}
# We intend to get a sample that has roughly the same distribution of values for any of these features but some of these columns vary to a high degree than others 

summary(iris[sample.index,])

```
```{r}
# can use the stratified functio but not available with my version of R 
library(dplyr)
library(rsample)
set.seed(123)
strata <- initial_split(iris, prop = 0.7, strata = "Sepal.Length")

train_data <- training(strata)
test_data <- testing(strata)

# View the first few rows of the training data
head(train_data)

summary(train_data)
# The stratifed sample has just about the same value
```

```{r}
# but you can you can specify which particular strata that I want to sample over, if youre sampling over many strata you generally want to start with the features that vary the least and then work way upward 
# those with the lowest variance include sepal.width and sepal.length


# again, should use stratifed function but not compatible with my R
iris <- iris %>%
  mutate(Combined_Strata = paste(Sepal.Width, Petal.Width, sep = "_"))

# Perform stratified random sampling using the combined strata
set.seed(123) # Set seed for reproducibility

split <- initial_split(iris, prop = 0.7, strata = Combined_Strata)

# Extract training (70%) and testing (30%) datasets
train_data <- training(split)
test_data <- testing(split)

# View the first few rows of the training data
head(train_data)

summary(train_data)


```

The means and variances returned here look pretty close to the population data!

# Systematic Sampling
## You can write a function that selects every nth row sequentially given some random intialization number 

```{r}
sys.saample = function(N, n) { # define function named sys.sample that takes the arguments N: total population size and n: desired sample size 
  k = ceiling(N/n) # (calculates the interval length) k, which is the step size for systematic sampling, k is computed as the ceiling of the ratio of the population size (N) to the sample size (n), using ceiling() ensures that k is rounded to the nearest whole number which avoids skipping elements when N isnt perfectly divisible by n  
  r = sample(1:k,1) # (random start point) randomly selects a starting point r from the first k element, this picks one random integer from the sequence 1 to k, setting the random start within the first interval  
  sys.sample = seq(r, r + k *(n-1), k) # generates sample sequence using the starting point r, interval length k, and sample size n , seq(r, r+k*(n-1), k) creates a sequence starting a r ending at r + k * (n-1) with a step size of k
} # the sequence selects every kth element starting from r, resulting in a sample of size n 

# ceiling takes a singe numeric argument x and returns a numeric vector containing the smallest integers not less than corresponding elements of x 
```

Example Usage:
If you call sys.sample(100, 10), assuming N = 100 and n = 10:

k = ceiling(100/10) = 10
Suppose r = 3 is randomly selected.
The systematic sample sequence would be: seq(3, 3 + 10 * (10-1), 10), which evaluates to seq(3, 93, 10), resulting in [3, 13, 23, 33, 43, 53, 63, 73, 83, 93].



#Example Usage:
If you call 
sys.sample(100, 10) #assuming N = 100 and n = 10:

k = ceiling(100/10) = 10
Suppose r = 3 is randomly selected.
The systematic sample sequence would be: seq(3, 3 + 10 * (10-1), 10), which evaluates to seq(3, 93, 10), resulting in [3, 13, 23, 33, 43, 53, 63, 73, 83, 93].

# Training and testing 
When building a predictive model you need to see what the errors generated by the model are so that you can tune it appropriately 

## Two major assumptions when working with training/testing splits 
- The data is a fair representation of the actual processes that you want to model (the subset accurately reflects the population)
- The process that you want to model are relatively stable over time and that a model built with last months data should accurately reflect next months data 

## Roles of Training and Testing Sets
- Almost all unsupervised learning algorithms follow the format for splitting the data into a training and testing set and use the training set for model training 
  - The coefficients you get as a result of the modeling procedures are based entirely on thet training data and dont depend on the testing at all 

## Why make a test set?
- The valule of making a test set of data for modelling purposes is extremely important because if it worked well for training but crashed with testing it loses all of its predictive power and becomes nothing more than a statc report 
  - "must tweek parameter B by a small amout to fit it"
  
- *Classification and regressiont regression trees (CARTS)* can be so flexibile in their modelling capabilities that if the tree is large enough you can often get misleading predictions 
  - you might train a model and see that it gives you 100% accuracy which should cause of concern , you can use the test data to evaluate the predictive performance of the trees in the data to find the one with the lowest error 
*the test set acts not only to validate the data but as a way to select which form of the model you need depending on the algorithm at play* 


## Training and Test Sets: Regression Modelling 

```{r}
set.seed(123)
x <- rnorm(100,2,1) #generate random normal data , 100 random numbers from a normal distribution with a mean of 2 and sd of 1. variable x is assigned this vector of random numbers 
y = exp(x) + rnorm(5, 0, 2) # creates a response variable y using a transformation of x , exp(x) calculates the exponential of each value in x, transforming it non-linearly 
plot(x,y)

linear <- lm(y~x) #fitting the linear regression model 
abline(a = coef(linear[1], b = coef(linear[2], lty = 2))) # adding regression line using the intercept and slope from the linear model, lty sets type of line to dashed  

# randomized data with a linear fit attached, the linear fit comes close to fitting some data points but not all (the further you extend x out, the more likely it is that your linear fit wont approximate the data very well )


```
```{r}
summary(linear)
```

The above summary uses 100% of the simulated data as its training and looks at model performance, a R squared of 0.74 isnt great but lets try to split 70/30!

```{r}
data <- data.frame(x,y)
data.samples <- sample(1:nrow(data), nrow(data) * .70, replace = FALSE)

training.data <- data[data.samples,] #training everything in 70%
test.data <- data[-data.samples,] #testing everything in 30%
```

```{r}
train.linear <- lm(y ~ x, training.data) # running the linear regression on training
```

```{r}
train.output <- predict(train.linear, test.data) # predict the linear model on test data
```
 Here your model is comparing what the model thinks the answer should be given input x compared to the actual values in your test set given by y..for regression...depending on the data and waht kind of error analysis that you want to do specifically...use **root-mean-square error (RMSE)**
 
# RMSE
You take the output values that the model has provided for the training data input, subtract those by the y values that you have in the test data, square the values, divide those by the total number of observations and sum up all values, and take the square root
 
```{r}
RMSE.df = data.frame(predicted = train.output, actual = test.data$y,
                     SE = ((train.output - test.data$y)^2/length(train.output)))

head(RMSE.df)
```

 
```{r}
sqrt(sum(RMSE.df$SE))
```
 
The value of 6.9 for RMSE is this models error score. To see how good this number is compare it another RMSE. You can run the same logic on a function fit of one higher degree and see what kind of RMSE you get as an end result.

```{r}
train.quadratc <- lm(y ~ x^2 + x, training.data) #one higher degree 
quadratic.output <- predict(train.quadratc, test.data)
RMSE.quad.df = data.frame(predicted = quadratic.output, actual = test.data$y,
                          SE = ((quadratic.output - test.data$y)^2)/length(train.output))

head(RMSE.quad.df)

```
 

```{r}
sqrt(sum(RMSE.quad.df$SE))
```
 
The output shows that bumping up the polynomial degree fit by one to a quadratic helps to decrease the error in what the model is predicting (from the quadratic,output variable) compared to what the actual values are. This follows from the fact that the actual data you're plotting appears to fit very well by a quadratic anyway. 
 
The next step is to increase the polynomial degree even further and assess how it affects the RMSE:

```{r}
train.polyn <- lm(y~poly(x,4), training.data)
polyn.output <- predict(train.polyn, test.data)
RMSE.polyn.df = data.frame(predicted = polyn.output, actual = test.data$y,
                           SE = ((polyn.output - test.data$y)^2/length(train.output)))
head(sum(RMSE.polyn.df$SE))
```


The RMSE has gone up compared ot the quadratic fit case. This follows the same pattern of a higher degree polynomial overfitting the data. But a smaller RMSE shows that the model fits the data nicely and morea ccurate predictions.

A simple linear fit to data would mostly be under fit; this is when the fitted line as given by the coefficients output from the model dont quite line up with the data you have. Meaning that the line on the chart the represents the machinel earning model (simple linear regression) doesnt explain most of the data, its too simple a model. Difficult for linear regressionto overfit the model so we rarely use training and testing data to evaluate them. With a quadratic fit the model tends to conform to the shape of most of the data points and now the model is a bit more complex. If you were to add anoter couple of data points in the same shape it would look like fits well. 
  - By training a  ML model with a specific training sample and looking at the difference between that and the saved test data you can evaluate how well the ML fit the data 
  - The downside of this future explanatory power if we add a few more dat apoints the model shown wont fit them well at all and will have an increased error 
  
  
In a complex model fit scenario, where the model is too specific to the training data, new test data will most likely have a lot of errors when we evaluated the model output versus our test data 

- When you do a train/test validation on continuous data as in regression, you can choose from a series of statistical measures like RMSE but you want to compare the output values that the mdoel gives you 
- But you want to compare the output values that the model gives you based on a subset of data that you used to train the model to that of the data you held out for testing purposes 
- Should have a list of numbers for the model estimates and a list of numbers that are the actual values 
- Thank you can bubble up to some aggregate number and compare against other methods 

## Root mean square error(RMSE), Mean absolute error(MAE), Root relative square error(RRSE), Relative Absolute Error (RAE)
- For **RMSE and MAE**, we look at the average difference between the model output ypredicted and the values we have in our test set yactual. 
- These are compared to the same scale of our feature
- You can think of it like 1 point of error is a difference of 1 between predicted and yactual 
- We tend to multiply it by 100 to get something in the 1 to 100 range and convert it to a percentage as well 
- The denominators of the two equations tells us how much the feature deviates from its average value which is why they call them relative 
# Training and Test Sets: Classification Modeling 
You evaluate a classifications models performance by starting with a confusion matrix. In a simple form, it can take representation as a 2 x 2 matrix in whcih the model output predicted classes are compared to the actual classes and the count of the model output in the cells of the matrix 
**This informs us how many true positives, true negatives, false positives, and false negatives there are**

```{r}
iris.df <- iris
iris.df$Species <- as.character(iris.df$Species) # convert species feature to a character
iris.df$Species[iris.df$Species != "setosa"] <- "other" # anything in species feature not setosa name as other
iris.df$Species <- as.factor(iris.df$Species)
iris.samples <- sample(1:nrow(iris.df), nrow(iris.df) * 0.7, # take a sample thats 70%
                       replace = TRUE)
training.iris <- iris.df[iris.samples,]
testing.iris <- iris.df[-iris.samples, ]
```


```{r}
library(randomForest)
iris.rf <- randomForest(Species ~ ., data = training.iris)
iris.predictions <- predict(iris.rf, testing.iris)
table(iris.predictions, testing.iris$Species)
# In a binary class truth table there are two outcomes: either predicted values is some class or it is not, in this case you're focusing on whether the model predicted setosa class or something else, there are four values for the confusion table

# Below our model is extremely accurate 
```
## True Positive
The model predicted setosa class and got them right 
## True Negative
The model predicted other classes and got them right 
## False Positive
The model predicted setosa and got it wrong
## False Negative
The model predicted other cl asses and got it wrong 

- 54 is true postive (top right) 
- 0 is false negative (bottom left)
- 0 is false positive (top right)
- 27 is true negative (bottom right)

Predicted is at top of matrix and actual is vertical left of matrix 

Say that other that (moving left to right top to bottom it was 28, 3, 2, 12) This forcees two false positives and two false negatives, we have 15 true positives (TP), 26 true negatives (TN), 2 false positives (FP), and 2 false negatives (FN)

- 28 is true positive 
- 2 is false negative 
- 3 is false positive
- 12 is true negative 

*now 15 true positives (3 + 12), 26 true negatives (28-2), 2 false positives, and 3 false negatives  
We have a number of statistics from which we can choose to test our accuracy:
Sensitivity: similar to hit rate or recall
Sensitivity: TP /(TP + FN) = 0.83
Specificity: TN / (TN + FP) = 0.92
Precision: positive prediction value 
Precision : TP / TP + FP = 0.99
Accuracy : TP + TN/ TP + TN + FP + FN = 0.89
F1 score : 2TP / 2TP + FP + FN = 0.86

Sensitivity: This is if you have a lower threshold set for your classification model, you would set a lower bar if you didnt want to miss out on any plants that could possibility be of a setosa type

Specificity: Logically the same thing as precision, but for the opposite case when you're predicting whether a plant isnt setosa variant

Precision: The number of positive cases you've predicted divided by the total predicted positive. If you had a model that had a very high sensitivity, that would be akin to setting a threshold in your model to say "only classify a plant as setosa if we are absolutely sure about it" (if given positive probabilty that it actually is)

Accuracy: the number of true cases divided by the total true and false cases 

F1 score: weighted average of precision and recall scores 

*When you have an almost identail number of false positives and false negatives using accuracy is the perfect measure to use 
*However if false positive or false negatives are skewed in favor of one or the other you need a more robust statistical test to account for such behavior
* f1 and accuracy are about the same number when the false positive and false negative rates are low  you can use usefulness of the F1 score if you look at a few different mock models that have various precision and recall values, might just take the average of the precision 


# Cross Validation 
So far we've tabled about how just running a model on 100% of your data could yield a result that dofesnt generalize well to new incoming data but the procecss of splitting the data is still somewhat limited 
  - when youre testing the model output against the reserved data  you are seeing only what the error is for that exat grouping of the test data 
  - the test data should be representative of the entire dataset 
  - *you want to train the model in such a way that you can be sure the error is representatiec of the entire dataset not just the specific slice you get from the randomly selected bits you put in the test data* 

## Cross validation : a statistical technique by which you take your entire dataset split it into a number of small train/test chunks, evaluate the error for each chunk, and then average those final errors. 
- The simple 70/30 train/test spliut is called a simple holdout cross validation technique 

## k fold cross validation 
- taking your dataset and splitting it into k chunks, for each chunk you then split the data into a smaller train/test set and evaluate the individual chunks error, after you have all the errors for all the chunks you simply take the average

You can use the *cut function* to evenly split up data for a given dataset's indices for subsetting and then loop over the applied folds of your data, doing train/test for each split
```{r}
set.seed(123)  # random number generator 
x <- rnorm(100, 2, 1)# generates 100 random numbers from a normal distributions with a mean of 2 and a sd of these numbers are stored in the variable x
y = exp(x) + rnorm(5, 0, 2) # generates the y by taking the exponential of x by adding 5 random numbers  from a normal distribution with a mean of 2 and a sd of 1
data <- data.frame(x, y) #combines vectors x and u into a df called data each row of df corresponds to a pair of values from x and y

data.shuffled <- data[sample(nrow(data)), ] # shuffles the rows of the df randomly storing results in new df and sample(nrow() function generates a random permutation of row indices ))
folds <- cut(seq(1, nrow(data)), breaks = 10, labels = FALSE) # divdes data into 10 folds for cross valdation and creates aa sequence from 1 to the. number of rows in data and cut assins each row to one of the 10 folds returning an integer label for each fold 

errors <- c(0) #initialize error values with a single element 0 which will later store the caluclated errors from each fold during cross val 

for(i in 1:10){ # starts a loop that iterates over each of teh 10 folds 
  fold.indexes <- which(folds == i, arr.ind = TRUE) # fold indices of the rows that belong to the current fold, fold i, and stores them in fold.indices 
  test.data <- data[fold.indexes, ] #extracts rows that correspond to current fold incides and stores in test
  training.data <- data[-fold.indexes, ] # creates training by exluding testing
  
  train.linear <- lm(y ~ x, training.data) # fits a linear model to training predicting y based on x 
  train.output <- predict(train.linear, test.data) # uses trained linear model to predict the y values for test data storing data 
  errors <- -c(errors, sqrt(sum(((train.output - test.data$y)^2/
                                 length(train.output)))))} # calculates RMSE of predicitons and appends it to the error vectors the involes subtracting predicted from actual in test.data$y, squarin gthe differnes, dividing by total number of predictions (lengh of train.output), taking square root of sum of squared differences

errors[2:11] # extracts RMSE values ignoring initial 0 from errors vector for the 10 folds 

mean(errors[2:11]) # calculates and returns the mean RMSE across all folds providing an overall estimate of the model performance 

```


This example shows that the errore stimate can vary to a wide degree within data depending on how you split data,  you can see the RMSE for 10 different cuts of the data, some , some errors go as low as 3.97  others as high as 10.5 so by using cross val not only can you see there is a high degree of variaibility in RMSE but you can mitigate that by taking the average of those values to get a number that mores representative of the error across tehe data as a whole 

# Summary 

