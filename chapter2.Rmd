---
title: "Supervised and Unsupervised Machine Learning"
author: "Kate Becker"
date: "2024-08-27"
output: html_document
---

```{r}
library(ggplot2)
library(tidyverse)
```

# In general: 
## Supervised learning models
- A machine learning model is scored and tuned against some sort of known quantity 
- Majority of machine learning algorithms are supervised learners 

Ex: built a model that says "any business that sells less than 10- units is a poor performer and more than 10 units is a good performer, we then have a set of data we want to test against that statement, suppose we have a set of data that sells 8 units this is less than 10 so therefore classifed as a poor performer 
- in this situation we have a model that ingests data in which were interested and gives us an output as decided by the conditions in the model 

## Unsupervised learning models:
- machine learning models derives patterns and information from data while determining
the known quantity tuning parameter itself 
ex: we have a bunch of data and want to know how to separate it into meaningful groups, we could have a bunch of data about peoples heights and weights, can use algorithms in the unsueprvised branch to figure out a way to group the data into meaningful clusters for which we might define clothing size
- in this case the model doesnty have an answer telling it "for this persons given height and wight, I should classify them as a small pant size" it must figure that out for itsef 

# Supervised 

# 3 major flavors 
### Regression 
- Very commong models, primarily used for looking at how data volves with respect to another variable (time) and examining what you can do to predict values in the future

### Classification 
- Used to reorganize data into schemes that make categorical sense, consider the aforementioned store labeling examples - stores that sell more than 10 units per week cold be classified as good performers, whereas those selling fewer than that number would be classified as poor 

### Mixed
- These models can often rely on parts of regression to inform how to do classification or sometimes the opposite. One case might be looking at sales data over time and wheher there is a rapid change in the slope of the line in some time period. 


# Regression
- We fit data that has an x and y elemnent, use an equation to predict what the correspodning output,y, should be for any given output x  (always done on numeric data)
```{r}
head(mtcars)
#11 features 

plot(y = mtcars$mpg, x = mtcars$disp, xlab = "Engine size (cubic inches)", ylab = "Fuel efficiency (miles per gallon)")
# Fuel efficiency decreases as the size of the engine increases but if you have some new engine for which you want know the efficiency it doesnt give you an exact answer 

model <- lm(mtcars$mpg ~ mtcars$disp)
coef(model)
```
*fuel efficiency = -0.041 x engine size + 29.599 

or call coefficients from the model directly

```{r}
coef(model)[2] * 200 + coef(model)[1]
```

# Training and Testing Data
- One way to determine model accuracy is to look at R-squared value from a model:

```{r}
summary(model)
```

The accuracy parameter that's most important is **adjusted R^2** that tells us how linearly correlated the data is, the closer the value is to one, the more likely the model output is governed by data that's almost exactly a straight line with some kind of slope to its value. 
- not focused on multiple is for future scenarios in which we use more features in a model, for low number of features the adjusted and multiple R squared values are basically the same
- for models with many features we want to use multiple r^2 values instead becasue it will give a more accurate assessment of the model errors if we have many dependant features instead of just one 
- for error estimate of the mode: 
  - we have standard error values from the output, but theres an issue with the model being trained on all of the data, then being tested on the same data 
- in order to ensure an unbiased amount of error, must split our data into training and testing
- split 80% training and 20% testing (always want more training than testing data)
```{r}
split_size = 0.8
sample_size = floor(split_size * nrow(mtcars)) # floor() takes a simple numeric argument x and returns a numeric vector containing the integers formed by truncating the values in x toward 0
set.seed(123) # for randomization 
train_indices <- sample(seq_len(nrow(mtcars)), size = sample_size) #sample takes a sample of specified size, seq_len() generates regular sequences of mtcar rows that are sample size of training 

train <- mtcars[train_indices, ]
test <- mtcars[-train_indices, ]
```

Code explanation: 
- sets the split size at 80% and then the sample size for training set to be 80% of the total number of rows 
- set a seed for reproducibility then get a list of row indices that we are going to put in our training data 
- then split the training and test data by setting the training data to be the rows that contain those indices and the test data is everything else 
- ! we can also use split(), test(), and train()

- build a regression model using only training data then pass it the test values to get the model outputs the key component is that we have the known data against which we can test the model allowing us to get a **a better level of error estimate out**

```{r}
model2 <- lm(mpg~disp, data = train) # runs model on training data
new.data <- data.frame(disp = test$disp) # creates new dataframe with the disp feature from test data
test$output <- predict(model2, new.data) # using the model on the test disp feature 
sqrt(sum(test$mpg - test$output)^2/nrow(test))
```
If were to look at residual standard error before you would see a different value however this value is deceiving because it was created using the same values as the testing 

Therefore: 
- We csplit the original mtcars dataset into a training that we use exlusivey for making the model and a test set which we use to test against it 
-  calculate new model using lm(), next form a df from oour tests disp column 
- then make predictions on our test set and store that in a new col in our test set 
- then find the RMSE (root mean square error) by taking the difference between our model output and the known mpg efficinecy, squaring it, summing up those squares, and dividing by the total number of entries in the dataset 
- the new value is different from what weve seen before and is important for understanding how well the model is performing! 

# Classification: 
Rather than predicting continuous values, like numbers, in classifcation exercises we'll predict discrete values 

## Logistic regression: sometimes you want to see if a given ata point is of categorical nature instead of numeric 

```{r}
plot(x = mtcars$mpg, y = mtcars$am, xlab = "Fuel Efficiency (Miles per Gallon)", ylab = "Vehicle Transmission Type (0 = automatic, 1 = manual)")
```
In the mtcars dataset each car is given a 0 or a 1 to determine whether it has an automatic transmission as defined by the column name am 
- A car with an automatic has a value of 1 and manual is assigned 0 
- Fitting a linear regression model to this data would not work becasue we cant have half a transmission value 
* We need to rely on logisitic regression model to help classify whether new efficiency data belongs to either the automatic or manual transmission groups 

The new question: How is the fuel efficieny related to a cars transmission type? 
- We could fit a regression line to the data but the results would be super misleading 
- Instead use a classification algorithm (logistic regression algorithm)

## Logistic regression: produces discrete outputs instead of continuous ones, expect a binary outcome 

```{r}
#install.packages("caTools")
library(caTools)
```

- The above library has a function for logistic regression :LogitBoost

```{r}
Label.train = train[, 9] #Need to give the model the label against which we want to predict as well as the data you want to use for training
Data.train = train[, -9] #"the data we want is the mtcars dataset that we split into a training set except column number 9" which is the am column we used before which subsets the data instead of listing out each column individually for input, just focuses on am!

model = LogitBoost(Data.train, Label.train) #set the label and data by picking the columns that represented each 
Data.test = test
Lab = predict(model, Data.test, type = "raw")
data.frame(row.names(test), test$mpg, test$am, Lab)
```
Here we have a given engine efficiency in mpg and a known value if the car is an automatic transmission (1) or not (0), then have two columns, x0, and x1, which are probabilities that are output by the model if thec ar is an automatic transission (x0) or a manual transmission (x1). Ways to tune this model to be more accurate could include colelcting more data in the training dataset or tuning the options available in the LogitBoots function itself 
ex: Mazda it comes out as an automatic and there is a 99% change it is a manual so this is wrong!

# Supervised Clustering Methdods
When you have a set of data and want to define classes based on how closely theyre grouped 
- A clustering algorithm can help you find patterns where they might otherwise be difficult to see explicitly 
- Good example of an ecosystem of algorithms that can be used both in unsupervised and supervised 
- One of the most popular forms of **Classification** and one of the most popular clustering models is the kmeans algorithm 
```{r}
plot(x = iris$Petal.Length, y = iris$Petal.Width, xlab = "Petal Length", ylab = "Petal Width")
# petal length as a function of petal width 
```
- Clumping of data in the lower left corner stands out as one of the obvious data clusters
- But how can we cluster the data in the above right portion into two groups? 
  - kmeans()
  
## Kmeans() 
### Works by first placing a number of random test points in our data and in this case two 
- Each of our real data points is measured as a distance from these test poiints and then the test points are moved in a way to minimze that distance 

```{r}
data = data.frame(iris$Petal.Length, iris$Petal.Width)
iris.kmeans <- kmeans(data, 2)
plot(x = iris$Petal.Length, y = iris$Petal.Width, pch = iris.kmeans$cluster,
     xlab = "Petal Length", ylab = "Petal Width")
points(iris.kmeans$centers, pch = 8, cex = 20)
```
Above the data is split into two major groups, in the lower left is one cluster denoted by small triangles and in the upper right is another cluster labeled with circular data points and the stars in the middle of each cluster mark where the cluster centers have stopped iterating 
- Any point that we further add to the daata is marked as being in a cluster if its closer to one vs another 
- There is one outlier data point in the lower left cluster so lets use one more cluster!

```{r}
iris.kmeans3 <- kmeans(data, 3)

plot(x = iris$Petal.Length, y = iris$Petal.Width, pch = iris.kmeans3$cluster,
     xlab = "Petal Length", ylab = "Petal Width")
points(iris.kmeans3$centers, pch = 8, cex = 2)

```
Now we have 3 groups to classify the dataset! The larger group of data has been split further into two clusters of data that look about equal in size!
- You can keep adding clusters but you would be losing important information (if every point was its own cluster it would be meaningless as far as classification goes!)
**Too few clusters and the data is underfit: there isnt a good way to determine structure**
**Too many clusters and you ave the opposite problem: there's far too much structure to make sense of simply**

```{r}

plot(x = iris$Petal.Length, iris$Petal.Width, pch = iris.kmeans$cluster, xlab = "Petal Length", ylab = "Petal Width", main = "Model Output")

plot(x = iris$Petal.Length, y = iris$Petal.Width, pch = as.integer(iris$Species), xlab = "Petal Length", ylab = "Petal Width", main = "Actual Data")

par(mfrow = c(1,2))
```

```{r}
head(iris)
unique(iris$Species) #3 species!
```

The three cluster kmeans algorithm works against the actual species labels in the data, seems to be a fairly good match! 

```{r}
table(iris.kmeans3$cluster, iris$Species)
```
You can read this confusion matrix with the output clusters as the rows, and the actual values from the data as the columns. 
  - Row 1: Setosa - all 50 setosa samples were classified correctly
  - Row 2: 48 vesicolor samples were correctly classified, 6 were misclassified as virginica
  - Row 3: 44 virginica samples were correctly classified, and 2 were misclassified as vesicolor 
*If the algorithm were 100% perfect we would expect the column to have all of its data in one of the three rows that pertains to the clusters just shows there were 8 off in cluster 2, 8 off in cluster 3 but none in cluster 1!

# Mixed Methods 
- We've discussed regression which takes in continous numeric data and aoutputs continous nuemric data 
- Classification which takes in continous numeric data and outputs discrete data or vice versa

## But some methods can use regression to help inform a classification scheme or data can be first taken as labels and used to constrain the regression models 

## Tree based models: 
### A tree is a structure that has nodes and edges
- For a decision tree: at each node we might have a value against which we split in order to gain some insights from the data 
```{r}
install.packages("party")
library(party)
tree <- ctree(mpg ~ ., data = mtcars)
plot(tree)

```
 ## Plotted Confidential Inference Tree
 Plotting engine fuel efficiency (mpg) but using all features in the dataset to build the model instead of just one; hence the mpg ~ . call in the ctree() function, the output is a distirbution (in the form a box and whisker plot) of the fuel efficiency as a function of the major features that influence it 

The **ctree()** function calls on certain methods to figure these out so you dont have a bunch of branches in the tree that dont amount go anything other than to clog the view there **the most important features to mpg are disp (enginge displacement) and wt(cars weight)** read the chart from the top to bottom 
- At node 1: there is a split for cars that weigh less than 2.32 tons and those that weigh more, for the cars that weight more we split further on enging displacement
- At node 3, for enginge displacements that have more than 258 cubic inches, we go to node 5
- For each feature there is a statistical p-value which determines how statistically relevant the feature is 
- The closer the p-value is to 0.05 or greater, the less useful or relevant it is so in this case the a p-value of almost exactly 0 is very good and likewise you can see how many data points make up each class at the bottom of the trees

Ex: lets consider a car that has a weight of four tons and a small engine size of 100 cubic inches so we would expect the fuel efficiency (at node 4) to be between 13 and 25 miles per gallon 

Now we want to use this structure for prediction?
The first thing that should pop up is you are looking at the entire data set instesd of just the training data 

```{r}
tree.train <- ctree(mpg~ ., data = train)
plot(tree.train)
```
By taking the same data and splitting it into a training set, you simplify the picture somewhat. The tree depends only on the cyl variable 

The code below performs both a regression and a classification test in two easy lines of code, first it takes the predict() function and applies it to the entirety of the test data and then stores it as a column in the test data then it performs the same procedure but add type="node" option to the predict() function to get a class out, then sticks them all together in a single df
```{r}
test$mpg.tree <- predict(tree.train, test) #predict() function and applies it to the entirety of the test data
test$class <- predict(tree.train, test, type = "node") #applies it to the entirety of the test data and then stores it as a column in the test data then it performs the same procedure but add type="node" option to get a class out!
data.frame(row.names(test), test$mpg, test$mpg.tree, test$class) # sticks them together in a df

```

We provided both a continuous, numeric output (regression) as well as a discrete class output (classification) for the same input


# Random Forests
ex: when considering a decision tree, a friends asking you a series of questions to fine tune what kind of movei you want to watch like is it two hours, does it have this actor etc.., and they recommend a movie based on these inputs but that's one friends input you want to ask a bunch of your friends and you go through the questions again and then they vote if you're interested in the movie 
- by asking a few frieds instead of one you build an ensemble classifier or a forest 
ex: you tell amanda you saw the dark knight eight times in theaters but maybe there was a reason why (saw with friends scheduling, etc.) that view count could be inflated so maybe the friends you ask should exlude that example 
- you tell amanda you cried during the movie armegeddon but only once while cutting an onion so you should weigh the movie less, instead of working with the same data set you vary it slightly 
- you aren't changing the end results of liking the movie or not but are tinkering with the decision that led to the result this is creating a **bootstrapped** version of your beginning data 

ex: Robert suggests the rock because he thinks you like jerry buckheimer movies more than you really do, max suggests Kagemusha, adn will thinks you wont like any of his results and thus recommendds nothing 
- these results are the **aggregated bootstrap forest** of movie preferences, your friends have no become a **Random forest**

*Random forests arent as easily describale in a model form as a simple y = mx+b or a simple tree that has a few nodes but you can do usual training and testing of continous and discrete data like we have seen with ctree()

```{r}
install.packages("randomForest")
library(randomForest)
mtcars.rf <- randomForest(mpg~ ., data = mtcars, ntree= 1000, keep.forest = FALSE, importance = FALSE)
plot(mtcars.rf, log = "y", title = "")

# Can show how the error in the model evolves over the course of how many trees were introduced into the model 
```

This graph shows the constraiing of error in a random forest algorithm with 1000 trees used this is as if **you had 1000 friends playing the movie guessing game for recommendations, you can see the error goes down with the more trees that you use and is minimal at around the n = 500 trees area) 

# Neural Networks 
### Takes in computational form the way neuros in a biological system work : For a given list of inputs, a neural network performs a number of processing steps before returning an output 
- The complexity in neural networks comes in how many of the processing steps there are and how complex each particular step may be 
- Exampleof how a neural network can work is through the use of logic gates 
- We use logical functions (ex: **an AND function is only true if both inputs are true, if one or both inputs are false the result is false**)

```{r}
TRUE & TRUE
TRUE & FALSE
FALSE & FALSE
```

### We can define a simple neural netwoek as one that takes in two inputs, calculates the AND function and gives a result which can be described in graphical form where you have layers and nodes 
**Layers** are vertical sections of the visual, and nodes are the points of computations within each layer 
- The mathematics of this require the use of **bias variable** which is a constant we add the equation for calculation purposes annd it represented as each node typically at the top of each layer in the neural network 
- In the case of the AND function we use numeric values passed into a classification function to give a value of 1 for TRUE and 0 for false and can do this using the sigmoid function : 

f(x) = 1 / (1 + e^-x)

- So for negative values of x that are less than -5 the function is basically 0 (denoinator gets closer to 0)
- For positive values of x that are greater than 5 the function is basically 1  (denominator gets closer to 1)
- If we had a predefined set of weights for each node in the neural network we could have three circiles in line vertically with (1,x1, x2) with arrows (-20, 15, 17) respectively pointing to one cricle and a line to f(x)
  - **we start with inputs X1, X2 and this bias node which is just an additive constant) (we calculate all these things at the empty circle which signifies a computation node, thec omputation taht we perform is putting all these things into an activation function which is almost always a sigmopid function, the output of the sigmoid function is the result of the neural networl**
  
Lets walk through this: 
- To calculate the end result of an AND gate (the f(x) on the ride side) we need to take in inputs for x1 and x2
- defining true to be 1 and false to be 0
- the last input we have is the bias variable which is 1 in the simple case 
- when the the network is trained we will find weights that are tied to each input 
- we then build an equation using those weights and find out what the equations result is 
- we then pass the result through a sigmoid function (the empty circle) and get the answer of the other side 

The weights we have are : -20, 15*x1, 17*x2 
- if x1 is TRUE it is a 1 otherwise it is a 0
- we then solve the equation and pass the final value through the sigmoid function and repeat this for all combinations of our input variables 

x1 = 1
x2 = 1
h(x) = f(-20 + 15 + 17)
h(x) = f(12) = 1

x1 = 1
x2 = 0
h(x) = f(-20 +15)
hx(x) = f(-5) = 0 

x1 = 0 
x2 =1
h(x) = f(-20 + 17)
h(x) = f(-3) = 0

x1 = 0
x2 = 0 
h(x) = f(-20) = 0 

- We started with a single layer of variables that have some predefined weight to them
- *The input layer* : a layer that takes in a number of features including a bias node which is often just a set of offset parameter 
- *The hidden layer or compute layer*: a layer that computes some function of each feature and the number of nodes in the hidden layer depends on the computations, sometimes may be as simple as one node in this layer or as complex as multiple hidden layers 
- *The output layer* a final processing node which might be a single function 

```{r}
set.seed(123)
library(nnet)
iris.nn <- nnet(Species ~., data = iris, size = 2)
# size = 2 option tells use that we using two hidden layers for computation which must specified 
# the output that we see are iterations of the network
```
We can use this for predictions: 
```{r}
table(iris$Species, predict(iris.nn, iris, type = "class"))
```

The result in the confusion matrix are the reference iris species of flowers across the top and the predicted iris species of flowers going up and down the table 
- We see that the neural network performed perfectly for classifying the setosa species but missed one classification for the versicolor and virgina species 
**A perfect machine learning model would have zeros for all the off diagonal elements**

# Support Vector Machines 

SVMs: you can use for both regression and classification can be simpler fo aster to a neural network 
- Similar to logistic regression 
- taking data and trying to find a plane or a line that can separate the data into different classes 

Suppose you have n features in your data and m observations or rows if n is much greater than m (n = 1000, m = 10) you would want to use a logisitic regressor and if you have the opposite you might want to use SVM instead 
- You can use a neural network but its considered slower 

```{r}
library(e1071)
iris.svm <- svm(Species ~., data = iris)
table(iris$Species, predict(iris.svm, iris, type = "class"))
```
The resutls look very similar to a neaural network but for the predicted species of flowers differed bu one compared to our nnet() classifier  

Drawbacks:
- Neural networks can be computationally expensive at scale or slow depending on complexitiy of the calculation 
- SVMs can be quicker but neural networks can represent more intelligent functions compared to the simplier SVM architecture 
- Neural networks handle multiple networks whereas SVMS can handle one at a time

# Unsupervised Learning
```{r}

```

