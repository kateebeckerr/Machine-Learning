---
title: "Supervised and Unsupervised Machine Learning"
author: "Kate Becker"
date: "2024-08-27"
output: html_document
---

```{r}
library(ggplot2)
library(tidyverse)
```

# In general: 
## Supervised learning models
- A machine learning model is scored and tuned against some sort of known quantity 
- Majority of machine learning algorithms are supervised learners 

Ex: built a model that says "any business that sells less than 10- units is a poor performer and more than 10 units is a good performer, we then have a set of data we want to test against that statement, suppose we have a set of data that sells 8 units this is less than 10 so therefore classifed as a poor performer 
- in this situation we have a model that ingests data in which were interested and gives us an output as decided by the conditions in the model 

## Unsupervised learning models:
- machine learning models derives patterns and information from data while determining
the known quantity tuning parameter itself 
ex: we have a bunch of data and want to know how to separate it into meaningful groups, we could have a bunch of data about peoples heights and weights, can use algorithms in the unsueprvised branch to figure out a way to group the data into meaningful clusters for which we might define clothing size
- in this case the model doesnty have an answer telling it "for this persons given height and wight, I should classify them as a small pant size" it must figure that out for itsef 

# Supervised 

# 3 major flavors 
### Regression 
- Very commong models, primarily used for looking at how data volves with respect to another variable (time) and examining what you can do to predict values in the future

### Classification 
- Used to reorganize data into schemes that make categorical sense, consider the aforementioned store labeling examples - stores that sell more than 10 units per week cold be classified as good performers, whereas those selling fewer than that number would be classified as poor 

### Mixed
- These models can often rely on parts of regression to inform how to do classification or sometimes the opposite. One case might be looking at sales data over time and wheher there is a rapid change in the slope of the line in some time period. 


# Regression
- We fit data that has an x and y elemnent, use an equation to predict what the correspodning output,y, should be for any given output x  (always done on numeric data)
```{r}
head(mtcars)
#11 features 

plot(y = mtcars$mpg, x = mtcars$disp, xlab = "Engine size (cubic inches)", ylab = "Fuel efficiency (miles per gallon)")
# Fuel efficiency decreases as the size of the engine increases but if you have some new engine for which you want know the efficiency it doesnt give you an exact answer 

model <- lm(mtcars$mpg ~ mtcars$disp)
coef(model)
```
*fuel efficiency = -0.041 x engine size + 29.599 

or call coefficients from the model directly

```{r}
coef(model)[2] * 200 + coef(model)[1]
```

# Training and Testing Data
- One way to determine model accuracy is to look at R-squared value from a model:

```{r}
summary(model)
```

The accuracy parameter that's most important is **adjusted R^2** that tells us how linearly correlated the data is, the closer the value is to one, the more likely the model output is governed by data that's almost exactly a straight line with some kind of slope to its value. 
- not focused on multiple is for future scenarios in which we use more features in a model, for low number of features the adjusted and multiple R squared values are basically the same
- for models with many features we want to use multiple r^2 values instead becasue it will give a more accurate assessment of the model errors if we have many dependant features instead of just one 
- for error estimate of the mode: 
  - we have standard error values from the output, but theres an issue with the model being trained on all of the data, then being tested on the same data 
- in order to ensure an unbiased amount of error, must split our data into training and testing
- split 80% training and 20% testing (always want more training than testing data)
```{r}
split_size = 0.8
sample_size = floor(split_size * nrow(mtcars)) # floor() takes a simple numeric argument x and returns a numeric vector containing the integers formed by truncating the values in x toward 0
set.seed(123) # for randomization 
train_indices <- sample(seq_len(nrow(mtcars)), size = sample_size) #sample takes a sample of specified size, seq_len() generates regular sequences of mtcar rows that are sample size of training 

train <- mtcars[train_indices, ]
test <- mtcars[-train_indices, ]
```

Code explanation: 
- sets the split size at 80% and then the sample size for training set to be 80% of the total number of rows 
- set a seed for reproducibility then get a list of row indices that we are going to put in our training data 
- then split the training and test data by setting the training data to be the rows that contain those indices and the test data is everything else 
- ! we can also use split(), test(), and train()

- build a regression model using only training data then pass it the test values to get the model outputs the key component is that we have the known data against which we can test the model allowing us to get a **a better level of error estimate out**

```{r}
model2 <- lm(mpg~disp, data = train) # runs model on training data
new.data <- data.frame(disp = test$disp) # creates new dataframe with the disp feature from test data
test$output <- predict(model2, new.data) # using the model on the test disp feature 
sqrt(sum(test$mpg - test$output)^2/nrow(test))
```
If were to look at residual standard error before you would see a different value however this value is deceiving because it was created using the same values as the testing 

Therefore: 
- We csplit the original mtcars dataset into a training that we use exlusivey for making the model and a test set which we use to test against it 
-  calculate new model using lm(), next form a df from oour tests disp column 
- then make predictions on our test set and store that in a new col in our test set 
- then find the RMSE (root mean square error) by taking the difference between our model output and the known mpg efficinecy, squaring it, summing up those squares, and dividing by the total number of entries in the dataset 
- the new value is different from what weve seen before and is important for understanding how well the model is performing! 

# Classification: 
Rather than predicting continuous values, like numbers, in classifcation exercises we'll predict discrete values 

## Logistic regression: sometimes you want to see if a given ata point is of categorical nature instead of numeric 

```{r}
plot(x = mtcars$mpg, y = mtcars$am, xlab = "Fuel Efficiency (Miles per Gallon)", ylab = "Vehicle Transmission Type (0 = automatic, 1 = manual)")
```
In the mtcars dataset each car is given a 0 or a 1 to determine whether it has an automatic transmission as defined by the column name am 
- A car with an automatic has a value of 1 and manual is assigned 0 
- Fitting a linear regression model to this data would not work becasue we cant have half a transmission value 
* We need to rely on logisitic regression model to help classify whether new efficiency data belongs to either the automatic or manual transmission groups 

The new question: How is the fuel efficieny related to a cars transmission type? 
- We could fit a regression line to the data but the results would be super misleading 
- Instead use a classification algorithm (logistic regression algorithm)

## Logistic regression: produces discrete outputs instead of continuous ones, expect a binary outcome 

```{r}
#install.packages("caTools")
library(caTools)
```

- The above library has a function for logistic regression :LogitBoost

```{r}
Label.train = train[, 9] #Need to give the model the label against which we want to predict as well as the data you want to use for training
Data.train = train[, -9] #"the data we want is the mtcars dataset that we split into a training set except column number 9" which is the am column we used before which subsets the data instead of listing out each column individually for input, just focuses on am!

model = LogitBoost(Data.train, Label.train) #set the label and data by picking the columns that represented each 
Data.test = test
Lab = predict(model, Data.test, type = "raw")
data.frame(row.names(test), test$mpg, test$am, Lab)
```
Here we have a given engine efficiency in mpg and a known value if the car is an automatic transmission (1) or not (0), then have two columns, x0, and x1, which are probabilities that are output by the model if thec ar is an automatic transission (x0) or a manual transmission (x1). Ways to tune this model to be more accurate could include colelcting more data in the training dataset or tuning the options available in the LogitBoots function itself 
ex: Mazda it comes out as an automatic and there is a 99% change it is a manual so this is wrong!

# Supervised Clustering Methdods
When you have a set of data and want to define classes based on how closely theyre grouped 
- A clustering algorithm can help you find patterns where they might otherwise be difficult to see explicitly 
- Good example of an ecosystem of algorithms that can be used both in unsupervised and supervised 
- One of the most popular forms of **Classification** and one of the most popular clustering models is the kmeans algorithm 
```{r}
plot(x = iris$Petal.Length, y = iris$Petal.Width, xlab = "Petal Length", ylab = "Petal Width")
# petal length as a function of petal width 
```
- Clumping of data in the lower left corner stands out as one of the obvious data clusters
- But how can we cluster the data in the above right portion into two groups? 
  - kmeans()
  
## Kmeans() 
### Works by first placing a number of random test points in our data and in this case two 
- Each of our real data points is measured as a distance from these test poiints and then the test points are moved in a way to minimze that distance 

```{r}
data = data.frame(iris$Petal.Length, iris$Petal.Width)
iris.kmeans <- kmeans(data, 2)
plot(x = iris$Petal.Length, y = iris$Petal.Width, pch = iris.kmeans$cluster,
     xlab = "Petal Length", ylab = "Petal Width")
points(iris.kmeans$centers, pch = 8, cex = 20)
```
Above the data is split into two major groups, in the lower left is one cluster denoted by small triangles and in the upper right is another cluster labeled with circular data points and the stars in the middle of each cluster mark where the cluster centers have stopped iterating 
- Any point that we further add to the daata is marked as being in a cluster if its closer to one vs another 
- There is one outlier data point in the lower left cluster so lets use one more cluster!

```{r}
iris.kmeans3 <- kmeans(data, 3)

plot(x = iris$Petal.Length, y = iris$Petal.Width, pch = iris.kmeans3$cluster,
     xlab = "Petal Length", ylab = "Petal Width")
points(iris.kmeans3$centers, pch = 8, cex = 2)

```
Now we have 3 groups to classify the dataset! The larger group of data has been split further into two clusters of data that look about equal in size!
- You can keep adding clusters but you would be losing important information (if every point was its own cluster it would be meaningless as far as classification goes!)
**Too few clusters and the data is underfit: there isnt a good way to determine structure**
**Too many clusters and you ave the opposite problem: there's far too much structure to make sense of simply**

```{r}

plot(x = iris$Petal.Length, iris$Petal.Width, pch = iris.kmeans$cluster, xlab = "Petal Length", ylab = "Petal Width", main = "Model Output")

plot(x = iris$Petal.Length, y = iris$Petal.Width, pch = as.integer(iris$Species), xlab = "Petal Length", ylab = "Petal Width", main = "Actual Data")

par(mfrow = c(1,2))
```

```{r}
head(iris)
unique(iris$Species) #3 species!
```

The three cluster kmeans algorithm works against the actual species labels in the data, seems to be a fairly good match! 

```{r}
table(iris.kmeans3$cluster, iris$Species)
```
You can read this confusion matrix with the output clusters as the rows, and the actual values from the data as the columns. 
  - Row 1: Setosa - all 50 setosa samples were classified correctly
  - Row 2: 48 vesicolor samples were correctly classified, 6 were misclassified as virginica
  - Row 3: 44 virginica samples were correctly classified, and 2 were misclassified as vesicolor 
*If the algorithm were 100% perfect we would expect the column to have all of its data in one of the three rows that pertains to the clusters just shows there were 8 off in cluster 2, 8 off in cluster 3 but none in cluster 1!

# Mixed Methods 
- We've discussed regression which takes in continous numeric data and aoutputs continous nuemric data 
- Classification which takes in continous numeric data and outputs discrete data or vice versa

## But some methods can use regression to help inform a classification scheme or data can be first taken as labels and used to constrain the regression models 

## Tree based models: 
### A tree is a structure that has nodes and edges
- For a decision tree: at each node we might have a value against which we split in order to gain some insights from the data 
```{r}
install.packages("party")
library(party)
tree <- ctree(mpg ~ ., data = mtcars)
plot(tree)

```
 ## Plotted Confidential Inference Tree
 Plotting engine fuel efficiency (mpg) but using all features in the dataset to build the model instead of just one; hence the mpg ~ . call in the ctree() function, the output is a distirbution (in the form a box and whisker plot) of the fuel efficiency as a function of the major features that influence it 

The **ctree()** function calls on certain methods to figure these out so you dont have a bunch of branches in the tree that dont amount go anything other than to clog the view there **the most important features to mpg are disp (enginge displacement) and wt(cars weight)** read the chart from the top to bottom 
- At node 1: there is a split for cars that weigh less than 2.32 tons and those that weigh more, for the cars that weight more we split further on enging displacement
- At node 3, for enginge displacements that have more than 258 cubic inches, we go to node 5
- For each feature there is a statistical p-value which determines how statistically relevant the feature is 
- The closer the p-value is to 0.05 or greater, the less useful or relevant it is so in this case the a p-value of almost exactly 0 is very good and likewise you can see how many data points make up each class at the bottom of the trees

Ex: lets consider a car that has a weight of four tons and a small engine size of 100 cubic inches so we would expect the fuel efficiency (at node 4) to be between 13 and 25 miles per gallon 

Now we want to use this structure for prediction?
The first thing that should pop up is you are looking at the entire data set instesd of just the training data 

```{r}
tree.train <- ctree(mpg~ ., data = train)
plot(tree.train)
```
By taking the same data and splitting it into a training set, you simplify the picture somewhat. The tree depends only on the cyl variable 

The code below performs both a regression and a classification test in two easy lines of code, first it takes the predict() function and applies it to the entirety of the test data and then stores it as a column in the test data then it performs the same procedure but add type="node" option to the predict() function to get a class out, then sticks them all together in a single df
```{r}
test$mpg.tree <- predict(tree.train, test) #predict() function and applies it to the entirety of the test data
test$class <- predict(tree.train, test, type = "node") #applies it to the entirety of the test data and then stores it as a column in the test data then it performs the same procedure but add type="node" option to get a class out!
data.frame(row.names(test), test$mpg, test$mpg.tree, test$class) # sticks them together in a df

```

We provided both a continuous, numeric output (regression) as well as a discrete class output (classification) for the same input

# Random Forests
